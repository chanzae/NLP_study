{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2da219",
   "metadata": {},
   "source": [
    "# NLP_Tutorial01_Text_Preprocessing\n",
    "\n",
    "패키지 설치\n",
    ">- NLTK(영어 자연어 처리)\n",
    ">- konlpy(한글 자연어처리)\n",
    ">- kss(한글 문장 분리기)\n",
    "\n",
    "## Tokenization\n",
    "의미가 있는 단위(Token)으로 나누는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d444384e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문단 : in computer science, lexical analysis, lexing or tokenization is the process of convert a sequence of characters (such as in a computer program or web page) into a sequence of lexical tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, although scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming language, web pages, and so forth.\n",
      "문장\n",
      "0번째 문장 : in computer science, lexical analysis, lexing or tokenization is the process of convert a sequence of characters (such as in a computer program or web page) into a sequence of lexical tokens (strings with an assigned and thus identified meaning).\n",
      "1번째 문장 : A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, although scanner is also a term for the first stage of a lexer.\n",
      "2번째 문장 : A lexer is generally combined with a parser, which together analyze the syntax of programming language, web pages, and so forth.\n",
      "\n",
      "문장 : Hello! This is NLT tutorial.\n",
      "단어 : ['Hello', '!', 'This', 'is', 'NLT', 'tutorial', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ejcej\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt') #필요한 데이터 다운로드\n",
    "print()\n",
    "\n",
    "text = \"in computer science, lexical analysis, lexing or tokenization is the process of convert a sequence of characters (such as in a computer program or web page) into a sequence of lexical tokens (strings with an assigned and thus identified meaning). \\\n",
    "A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, although scanner is also a term for the first stage of a lexer. \\\n",
    "A lexer is generally combined with a parser, which together analyze the syntax of programming language, web pages, and so forth.\"\n",
    "\n",
    "print(f'문단 : {text}')\n",
    "print('문장')\n",
    "n = 0\n",
    "for sent in sent_tokenize(text): #문단을 문장으로 분리한 결과 하나씩 출력\n",
    "    print(f'{n}번째 문장 : {sent}')\n",
    "    n += 1\n",
    "print()\n",
    "\n",
    "sentence = 'Hello! This is NLT tutorial.'\n",
    "print(f'문장 : {sentence}')\n",
    "print(f'단어 : {word_tokenize(sentence)}') #문장을 단어로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c419af5",
   "metadata": {},
   "source": [
    "## KSS (Korean Sentence Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e073ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문단 : 안녕하세요. 만나서 반갑습니다.\n",
      "문장 : ['안녕하세요.', '만나서 반갑습니다.']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '안녕하세요. 만나서 반갑습니다.'\n",
    "print(f'문단 : {text}')\n",
    "print(f'문장 : {kss.split_sentences(text)}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb1d27",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "품사 태깅  \n",
    "\n",
    "---\n",
    "\n",
    "konlpy 공식 홈페이지 : https://konlpy.org/ko/latest/  \n",
    "한국어 형태소 분석기 : 한나눔, 꼬꼬마, 코모란, Mecab, Okt 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83b8482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문장 : I am a boy and you are a girl.\n",
      "단어 : ['I', 'am', 'a', 'boy', 'and', 'you', 'are', 'a', 'girl', '.']\n",
      "품사 : [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('boy', 'NN'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('girl', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "print()\n",
    "\n",
    "text = 'I am a boy and you are a girl.'\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print(f'문장 : {text}')\n",
    "print(f'단어 : {tokenized_sentence}')\n",
    "print(f'품사 : {pos_tag(tokenized_sentence)}') #각 단어의 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82bacc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문장 : 한글 문장도 형태소 분석이 가능할까요?\n",
      "\n",
      "형태소 분석(okt) : ['한글', '문장', '도', '형태소', '분석', '이', '가능할까', '요', '?']\n",
      "품사 태깅(okt) : [('한글', 'Noun'), ('문장', 'Noun'), ('도', 'Josa'), ('형태소', 'Noun'), ('분석', 'Noun'), ('이', 'Josa'), ('가능할까', 'Adjective'), ('요', 'Noun'), ('?', 'Punctuation')]\n",
      "명사 추출(okt) : ['한글', '문장', '형태소', '분석', '요']\n",
      "\n",
      "형태소 분석(kkma) : ['한글', '문장도', '형태소', '분석', '이', '가능', '하', 'ㄹ까요', '?']\n",
      "품사 태깅(kkma) : [('한글', 'NNG'), ('문장도', 'NNG'), ('형태소', 'NNG'), ('분석', 'NNG'), ('이', 'JKS'), ('가능', 'NNG'), ('하', 'XSV'), ('ㄹ까요', 'EFQ'), ('?', 'SF')]\n",
      "명사 추출(kkma) : ['한글', '문장도', '형태소', '분석', '가능']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt,Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "kor_text = \"한글 문장도 형태소 분석이 가능할까요?\"\n",
    "print()\n",
    "\n",
    "print(f'문장 : {kor_text}')\n",
    "print()\n",
    "print(f'형태소 분석(okt) : {okt.morphs(kor_text)}')\n",
    "print(f'품사 태깅(okt) : {okt.pos(kor_text)}')\n",
    "print(f'명사 추출(okt) : {okt.nouns(kor_text)}')\n",
    "print()\n",
    "\n",
    "print(f'형태소 분석(kkma) : {kkma.morphs(kor_text)}')\n",
    "print(f'품사 태깅(kkma) : {kkma.pos(kor_text)}')\n",
    "print(f'명사 추출(kkma) : {kkma.nouns(kor_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182d80b",
   "metadata": {},
   "source": [
    "**+) NLTK POS Taglist**  \n",
    "\n",
    "| Abbreviation | Meaning |  \n",
    "|:---:|:---|  \n",
    "|CC|coordinating conjunction|  \n",
    "|CD|cardinal digit|  \n",
    "|DT|determiner|  \n",
    "|EX|existential there|  \n",
    "|FW|foreign word|  \n",
    "|IN|preposition/subordinating conjunction|  \n",
    "|JJ|This NLTK POS Tag is an adjective (large)|  \n",
    "|JJR|adjective, comparative (larger)|  \n",
    "|JJS|adjective, superlative (largest)|  \n",
    "|LS|list market|  \n",
    "|MD|modal (could, will)|  \n",
    "|NN|noun, singular (cat, tree)|  \n",
    "|NNS|noun plural (desks)|  \n",
    "|NNP|proper noun, singular (sarah)|  \n",
    "|NNPS|proper noun, plural (indians or americans)|  \n",
    "|PDT|predeterminer (all, both, half)|  \n",
    "|POS|possessive ending (parent\\ ‘s)|  \n",
    "|PRP|personal pronoun (hers, herself, him, himself)|  \n",
    "|PRP$|possessive pronoun (her, his, mine, my, our )|  \n",
    "|RB|adverb (occasionally, swiftly)|  \n",
    "|RBR|adverb, comparative (greater)|  \n",
    "|RBS|adverb, superlative (biggest)|  \n",
    "|RP|particle (about)|  \n",
    "|TO|infinite marker (to)|  \n",
    "|UH|interjection (goodbye)|  \n",
    "|VB|verb (ask)|  \n",
    "|VBG|verb gerund (judging)|  \n",
    "|VBD|verb past tense (pleaded)|  \n",
    "|VBN|verb past participle (reunified)|  \n",
    "|VBP|verb, present tense not 3rd person singular(wrap)|  \n",
    "|VBZ|verb, present tense with 3rd person singular (bases)|  \n",
    "|WDT|wh-determiner (that, what)|  \n",
    "|WP|wh- pronoun (who)|  \n",
    "|WRB|wh- adverb (how)|  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff4c57",
   "metadata": {},
   "source": [
    "## 표제어 추출 및 어간 추출\n",
    "### 표제어 : 기본 사전형 단어(단어의 뿌리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f27859d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'dy', 'watched', 'ha', 'starting']\n",
      "\n",
      "품사 정보 추가(doing) :  do\n",
      "품사 정보 추가(dies) :  die\n",
      "품사 정보 추가(watced) :  watch\n",
      "품사 정보 추가(has) :  have\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ejcej\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives',\\\n",
    "         'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "#표제어 추출\n",
    "print(f'단어 : {words}')\n",
    "print(f'표제어 : {[lemmatizer.lemmatize(word) for word in words]}') #표제어 추출\n",
    "print()\n",
    "\n",
    "# doing, dies, has 같은 경우는 제대로 표제어를 추출해내지 못함. 따라서 이 단어들에 대해 정보 추가\n",
    "# 품사 정보 추가\n",
    "print('품사 정보 추가(doing) : ',lemmatizer.lemmatize('doing', 'v'))\n",
    "print('품사 정보 추가(dies) : ',lemmatizer.lemmatize('dies', 'v'))\n",
    "print('품사 정보 추가(watced) : ',lemmatizer.lemmatize('watched','v'))\n",
    "print('품사 정보 추가(has) : ',lemmatizer.lemmatize('has', 'v'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff069b4",
   "metadata": {},
   "source": [
    "### 어간 : 단어의 의미를 담고 있는 핵심 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e9ca5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 : ['In', 'linguistic', 'morphology', 'and', 'information', 'retrleval', ',', 'stemming', 'is', 'the']\n",
      "어간(Porter) : ['in', 'linguist', 'morpholog', 'and', 'inform', 'retrlev', ',', 'stem', 'is', 'the']\n",
      "어간(Lancaster) : ['in', 'lingu', 'morpholog', 'and', 'inform', 'retrlev', ',', 'stem', 'is', 'the']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "sentence = \"In linguistic morphology and information retrleval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form-generally a written word form.\"\n",
    "tokenized_sentence = word_tokenize(sentence)[:10] # 10개 단어\n",
    "\n",
    "#어간추출\n",
    "print(f'단어 : {tokenized_sentence}') #10개 단어\n",
    "print(f'어간(Porter) : {[porter.stem(word) for word in tokenized_sentence]}')\n",
    "print(f'어간(Lancaster) : {[lancaster.stem(word) for word in tokenized_sentence]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00a95f",
   "metadata": {},
   "source": [
    "## 불용어\n",
    "큰 의미가 없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "833b452a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nltk 불용어 개수 : 179\n",
      "불용어 예시 : ['i', 'me', 'my', 'myself', 'we']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ejcej\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print()\n",
    "\n",
    "stop_words_list = stopwords.words('english') # nltk영어의 불용어 리스트\n",
    "print(f'nltk 불용어 개수 : {len(stop_words_list)}')\n",
    "print(f'불용어 예시 : {stop_words_list[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a24eac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 단어 : ['이', '문장', '에서', '불', '용어', '를', '제외', '하면', '무엇', '이', '남을까요', '?']\n",
      "불용어 제거 : ['문장', '불', '용어', '제외', '무엇', '남을까요', '?']\n"
     ]
    }
   ],
   "source": [
    "example = '이 문장에서 불용어를 제외하면 무엇이 남을까요?'\n",
    "stop_words = '이 에서 를 하면'\n",
    "\n",
    "# 조사, 접속사 제저\n",
    "stop_words = stop_words.split(' ') # 띄어쓰기 기준으로 문장 분리\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words] #불용어에 해당하는 경우 제거\n",
    "\n",
    "print(f'원래 단어 : {word_tokens}')\n",
    "print(f'불용어 제거 : {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883903c",
   "metadata": {},
   "source": [
    "## 정규표현식\n",
    "\n",
    "### 정규표현식 문법\n",
    "|식별자|예제 | 설명|\n",
    "|:---:|:---:|:---|\n",
    "| . | a.c | 임의의 문자 1개(와일드카드)|\n",
    "| ? | a?c | 문자가 존재하거나 아닌 경우|\n",
    "| * | ab\\*c | 문자가 0개 이상 존재하는 경우|\n",
    "| + | ab+c | 문자가 1개 이상 존재하는 경우|\n",
    "| ^ | ^a | 특정 문자열로 시작|\n",
    "| $ | c\\$ | 특정 문자열로 종료|\n",
    "| {} | ab{2}c | 특정 숫자만큼 반복|\n",
    "| {2,3} | ab{2,3}c | 특정 범위만큼 반복|\n",
    "| [] | [abc] | a 또는 b 또는 c가 문자열에 존재하는가?|\n",
    "| [^] | [^abc] | a 또는 b 또는 c가 문자열 제외 | \n",
    "| \\| | a\\|bc | or 연산자, a 또는 bc가 존재하는가? |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a6f55ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc : <re.Match object; span=(0, 3), match='abc'>\n",
      "abbc : None\n",
      "acd : None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#임의의 문자 1개 : .\n",
    "re1 = re.compile(\"a.c\") # a와 c 사이에 아무 문자 1개 ex) acc, abc,...\n",
    "\n",
    "print(f'abc : {re1.search(\"abc\")}') # a와 c사이에 b존재 = True\n",
    "print(f'abbc : {re1.search(\"abbc\")}') # a와 c사이에 bb존재 = None\n",
    "print(f'acd : {re1.search(\"acd\")}') # a와 c사이에 아무것도 존재하지 않음 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6486d376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc : <re.Match object; span=(0, 3), match='abc'>\n",
      "ac : <re.Match object; span=(0, 2), match='ac'>\n",
      "ab : None\n",
      "ab : None\n"
     ]
    }
   ],
   "source": [
    "#문자가 존재하거나 아닌 경우 : ?\n",
    "re2 = re.compile(\"ab?c\") # a와 c 사이에 b문자가 존재하거나 하지 않거나\n",
    "                        # ex) ac, abc\n",
    "\n",
    "print(f'abc : {re2.search(\"abc\")}') # a와 c사이에 b존재 = True\n",
    "print(f'ac : {re2.search(\"ac\")}') # a와 c사이에 아무것도 존재하지 않음 = True\n",
    "print(f'ab : {re2.search(\"ab\")}') # re2패턴이 존재하지 않음 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7998d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac : <re.Match object; span=(0, 2), match='ac'>\n",
      "abc : <re.Match object; span=(0, 3), match='abc'>\n",
      "abbd : <re.Match object; span=(0, 4), match='abbc'>\n",
      "abbdc : None\n"
     ]
    }
   ],
   "source": [
    "#문자가 0개 이상일 경우 : *\n",
    "re3 = re.compile(\"ab*c\") #a와 c 사이에 b문자가 0개 이상\n",
    "                        # ex) ac, abc, abbc, abbbbc, ....\n",
    "\n",
    "print(f'ac : {re3.search(\"ac\")}') # a와 c사이에 아무것도 존재하지 않음 = None\n",
    "print(f'abc : {re3.search(\"abc\")}') # a와 c사이에 b존재 = True\n",
    "print(f'abbd : {re3.search(\"abbc\")}') # a와 c사이에 bb존재 = True\n",
    "print(f'abbdc : {re3.search(\"abbdc\")}') # a와 c사이에 bbd존재 = None (d는 패턴이 아님)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb4554bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac : None\n",
      "abc : <re.Match object; span=(0, 3), match='abc'>\n",
      "abbd : <re.Match object; span=(0, 4), match='abbc'>\n",
      "abbdc : None\n"
     ]
    }
   ],
   "source": [
    "#문자가 1개 이상일 경우 : +\n",
    "re4 = re.compile(\"ab+c\") #a와 c 사이에 b문자가 1개 이상\n",
    "                        # ex) abc, abbc, abbbbc\n",
    "    \n",
    "print(f'ac : {re4.search(\"ac\")}') # a와 c사이에 아무것도 존재하지 않음 = None\n",
    "print(f'abc : {re4.search(\"abc\")}') # a와 c사이에 b존재 = True\n",
    "print(f'abbd : {re4.search(\"abbc\")}') # a와 c사이에 bb존재 = True\n",
    "print(f'abbdc : {re4.search(\"abbdc\")}') # a와 c사이에 bbd존재 = None (d는 패턴이 아님)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df4dd39f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : <re.Match object; span=(0, 1), match='a'>\n",
      "abc : <re.Match object; span=(0, 1), match='a'>\n",
      "da : None\n"
     ]
    }
   ],
   "source": [
    "#특정 문자열로 시작 : ^\n",
    "re5 = re.compile(\"^a\") # a로 시작하는 단어\n",
    "\n",
    "print(f'a : {re5.search(\"a\")}') # a로 시작 = True\n",
    "print(f'abc : {re5.search(\"abc\")}') # a로 시작 = True\n",
    "print(f'da : {re5.search(\"da\")}') # d로 시작 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "008f6190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc : None\n",
      "abbc : <re.Match object; span=(0, 4), match='abbc'>\n",
      "abbbc : None\n"
     ]
    }
   ],
   "source": [
    "#특정 숫자만큼 반복 : {}\n",
    "re6 = re.compile(\"ab{2}c\") # abbc\n",
    "\n",
    "print(f'abc : {re6.search(\"abc\")}') # abc = None\n",
    "print(f'abbc : {re6.search(\"abbc\")}') # abbc = True\n",
    "print(f'abbbc : {re6.search(\"abbbc\")}') # abbbc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b63f5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc : None\n",
      "abbc : <re.Match object; span=(0, 4), match='abbc'>\n",
      "abbbc : <re.Match object; span=(0, 5), match='abbbc'>\n",
      "abbbbc : None\n"
     ]
    }
   ],
   "source": [
    "#특정 범위만큼 반복 : {start_num이상, end_num이하}\n",
    "# 이때 start_num이나 end_num 둘 중 하나 생략해서 사용 가능 ex) {,3}, {2,}\n",
    "re7 = re.compile(\"ab{2,3}c\") # abbc\n",
    "\n",
    "print(f'abc : {re7.search(\"abc\")}') # abc = None\n",
    "print(f'abbc : {re7.search(\"abbc\")}') # abbc = True\n",
    "print(f'abbbc : {re7.search(\"abbbc\")}') # abbbc = True\n",
    "print(f'abbbbc : {re7.search(\"abbbbc\")}') # abbbbc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5006c9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ade : <re.Match object; span=(0, 1), match='a'>\n",
      "deb : <re.Match object; span=(2, 3), match='b'>\n",
      "dce : <re.Match object; span=(1, 2), match='c'>\n",
      "def : None\n",
      "\n",
      "a : <re.Match object; span=(0, 1), match='a'>\n",
      "d : <re.Match object; span=(0, 1), match='d'>\n",
      "z : <re.Match object; span=(0, 1), match='z'>\n",
      "y : <re.Match object; span=(0, 1), match='y'>\n"
     ]
    }
   ],
   "source": [
    "#특정 문자열 중 하나 : []\n",
    "re8 = re.compile(\"[abc]\") # a or b or c\n",
    "\n",
    "print(f'ade : {re8.search(\"ade\")}') # ade = Ture(a 존재)\n",
    "print(f'deb : {re8.search(\"deb\")}') # deb = True(b 존재)\n",
    "print(f'dce : {re8.search(\"dce\")}') # dce = True(c 존재)\n",
    "print(f'def : {re8.search(\"def\")}') # def = None\n",
    "print()\n",
    "\n",
    "#소문자 문자열 확인\n",
    "re8_az = re.compile(\"[a-z]\") # a - z 존재\n",
    "\n",
    "print(f'a : {re8_az.search(\"a\")}') # a = Ture\n",
    "print(f'd : {re8_az.search(\"d\")}') # d = True\n",
    "print(f'z : {re8_az.search(\"z\")}') # z = True\n",
    "print(f'y : {re8_az.search(\"y\")}') # y = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ece34628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc : None\n",
      "abd : <re.Match object; span=(2, 3), match='d'>\n"
     ]
    }
   ],
   "source": [
    "#특정 문자열 제외 : [^]\n",
    "re9 = re.compile(\"[^abc]\") # a또는 b 또는 c가 아닌 문자열\n",
    "\n",
    "print(f'abc : {re9.search(\"abc\")}') # abc = None(a,b,c 모두 존재)\n",
    "print(f'abd : {re9.search(\"abd\")}') # abd = True(d만 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f1a9f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab : <re.Match object; span=(0, 1), match='a'>\n",
      "bcd : <re.Match object; span=(0, 2), match='bc'>\n",
      "cc : None\n"
     ]
    }
   ],
   "source": [
    "#특정 문자열 중 하나 : |\n",
    "re10 = re.compile(\"a|bc\") #a 또는 bc 문자열이 존재하는가?\n",
    "\n",
    "print(f'ab : {re10.search(\"ab\")}') # ab = True(a 출력)\n",
    "print(f'bcd : {re10.search(\"bcd\")}') # bcd = True(bc 출력)\n",
    "print(f'cc : {re10.search(\"cc\")}') # cc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad20ec",
   "metadata": {},
   "source": [
    "### 정규표현식 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d58fc",
   "metadata": {},
   "source": [
    "|Method|Describe|\n",
    "|:---:|:---|\n",
    "|`.compile()` | 인자로 들어오는 문자열을 규칙으로 정의|\n",
    "|`.search()`|문자열 전체에 대해서 정규표현식과 매치되는지 검색|\n",
    "|`.match()`|문자열의 처음이 정규표현식과 매치되는지 검색|\n",
    "|`.split()`|정규표현식을 기준으로 문자열 분리 후 리스트로 반환|\n",
    "|`.findall()`|문자열에서 정규표현식과 매치되는 모든 문자열을 찾아서 리스트로 반환|\n",
    "|`.sub()`|문자열에서 정규표현식과 일치하는 부분을 다른 문자열로 대체|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd1aaa3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search(text1) : <re.Match object; span=(0, 2), match='ab'>\n",
      "search(text2) : <re.Match object; span=(1, 3), match='ab'>\n",
      "\n",
      "match(text1) : <re.Match object; span=(0, 2), match='ab'>\n",
      "match(text2) : None\n"
     ]
    }
   ],
   "source": [
    "r = re.compile('ab')\n",
    "text1 = 'abaab abb acb abab'\n",
    "text2 = 'babaab abb acb abab'\n",
    "\n",
    "#search()\n",
    "print(f'search(text1) : {r.search(text1)}')\n",
    "print(f'search(text2) : {r.search(text2)}')\n",
    "print()\n",
    "\n",
    "#match()\n",
    "print(f'match(text1) : {r.match(text1)}')\n",
    "print(f'match(text2) : {r.match(text2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a8294",
   "metadata": {},
   "source": [
    "search(), match()할 때 패턴을 compile()한 후 변수에 담아 search, match 해도 되지만, compile하지 않고 인자로 넣어 사용해도 된다.\n",
    "\n",
    "ex1)  \n",
    ">pattern = 'ab'  \n",
    ">re.search(pattern,text1)\n",
    "\n",
    "ex2)  \n",
    ">r = re.compile('ab')  \n",
    ">r.search(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90ba6272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split(text1) : ['abaab', 'abb', 'acb', 'abab']\n",
      "split(text2) : ['babaab', 'abb', 'acb', 'abab']\n"
     ]
    }
   ],
   "source": [
    "#split(정규표현식 규칙, 분리할 문자열)\n",
    "print(f'split(text1) : {re.split(\" \", text1)}') \n",
    "print(f'split(text2) : {re.split(\" \", text2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1b52e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findall(text1) : ['ab', 'ab', 'ab', 'ab', 'ab']\n",
      "findall(text2) : ['ab', 'ab', 'ab', 'ab', 'ab']\n"
     ]
    }
   ],
   "source": [
    "#findall(정규표현식 규칙, 분리할 문자열)\n",
    "print(f'findall(text1) : {re.findall(r, text1)}') \n",
    "print(f'findall(text2) : {re.findall(r, text2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6f14db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub(text1) : zaz zb acb zz\n",
      "sub(text2) : bzaz zb acb zz\n"
     ]
    }
   ],
   "source": [
    "#sub(정규표현식 규칙, 대체할 문자열, 대체될 문자열)\n",
    "print(f'sub(text1) : {re.sub(r, \"z\", text1)}') \n",
    "print(f'sub(text2) : {re.sub(r, \"z\", text2)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a333ca",
   "metadata": {},
   "source": [
    "**+) 정규표현식 패턴**\n",
    "\n",
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th><th>etc.</th></tr>\n",
    "\n",
    "<tr ><td><span >\\d</span></td><td>A digit</td><td>file_\\d\\d</td><td>file_25</td><td>'file_'뒤에 오는 모든 숫자는 다 가능</td></tr>\n",
    "\n",
    "<tr ><td><span >\\w</span></td><td>Alphanumeric</td><td>\\w-\\w\\w\\w</td><td>A-b_1</td><td>일련의 문자, 숫자</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\s</span></td><td>White space</td><td>a\\sb\\sc</td><td>a b c</td><td>공백문자</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\D</span></td><td>A non digit</td><td>\\D\\D\\D</td><td>ABC</td><td>숫자가 아닌 문자</td></tr>\n",
    "\n",
    "<tr ><td><span >\\W</span></td><td>Non-alphanumeric</td><td>\\W\\W\\W\\W\\W</td><td>*-+=)</td><td>알파벳이나 숫자가 아닌 문자 = 특수문자 같은 것</td></tr>\n",
    "\n",
    "<tr ><td><span >\\S</span></td><td>Non-whitespace</td><td>\\S\\S\\S\\S</td><td>Yoyo</td><td>공백이 아닌 모든 문자</td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5749ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규식 : 0.019571304321289062\n",
      "\n",
      "split+in : 0.026005268096923828\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# wikipedia 'python'의 첫 문단\n",
    "wiki_text = \"Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[32] \\\n",
    "Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.[33][34] \\\n",
    "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0.[35] Python 2.0 was released in 2000 and introduced new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 3.0, released in 2008, was a major revision that is not completely backward-compatible with earlier versions. Python 2 was discontinued with version 2.7.18 in 2020.[36] \\\n",
    "Python consistently ranks as one of the most popular programming languages.[37][38][39][40]\"\n",
    "n_copy = 1000 #처리해야할 text의 규모가 커질수록, 규칙이 복잡할수록 정규식이 더 빠름\n",
    "wiki_text = ' '.join([wiki_text] * n_copy)\n",
    "\n",
    "# 대문자로 시작하고 5글자 이하인 단어 찾기(단, I로 시작하는 단어 제외)\n",
    "# 1. 대문자 시작\n",
    "# 2. 5글자 이하\n",
    "# 3. I로 시작 제외\n",
    "\n",
    "# 정규식\n",
    "re_start = time.time()\n",
    "\n",
    "# I제외, 5글자 이하, 대문자 시작조건 모두 충족하는 규칙 생성\n",
    "pattern = re.compile(r\"\\b[A-HJ-Z]\\w{,4}\\b\") # \\b는 단어의 경계를 의미, \\w는 문자 또는 숫자\n",
    "re_list = re.findall(pattern, wiki_text)\n",
    "\n",
    "# print(re_list)\n",
    "\n",
    "re_end = time.time()\n",
    "\n",
    "print(f'정규식 : {re_end - re_start}')\n",
    "print()\n",
    "\n",
    "#split과 in\n",
    "split_in_start = time.time()\n",
    "word_list = wiki_text.split() # 문단을 단어로 분리\n",
    "upper_list = list() \n",
    "for word in word_list:\n",
    "    if word[0].isupper() and word[0] != 'I' and len(word) <=5:\n",
    "        upper_list.append(word)\n",
    "\n",
    "# print(upper_list)\n",
    "\n",
    "split_in_end = time.time()\n",
    "\n",
    "print(f'split+in : {split_in_end - split_in_start}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
